# ğŸ•µï¸ Shadow Chase - AI Agent Training & Evaluation Platform

> **âš–ï¸ Legal Notice**: This is an independent educational/research implementation inspired by the Scotland Yard board game. This project is not affiliated with or endorsed by Ravensburger AG (owners of Scotland Yard trademark). For educational and research use only.


A comprehensive implementation of the Shadow Chase pursuit-evasion game with multiple AI agents for research, learning, and experimentation with reinforcement learning and game theory techniques.

## ğŸ¯ Project Overview

This project aims to provide a platform for:
- **Learning Deep Q-Learning (DQN)** and comparing it to other AI techniques
- **Creating a testing environment** for various AI strategies and algorithms
- **Exploring game theory** in pursuit-evasion scenarios
- **Benchmarking different agent implementations** against each other

## ğŸ“‹ Table of Contents

### ğŸš€ Getting Started
- [How Shadow Chase Works](#-how-shadow-chase-works)
- [Quick Start](#-quick-start)
  - [Prerequisites](#prerequisites)
  - [Installation](#installation)
  - [Three Main Ways to Use This Project](#three-main-ways-to-use-this-project)

### ğŸ—ï¸ Project Structure & Documentation
- [Project Structure](#ï¸-project-structure)
- [Available Agent Types](#-available-agent-types)
- [Configuration](#-configuration)

### ğŸ® Usage & Examples
- [Usage Examples](#-usage-examples)
- [Examples & Tutorials](#-examples--tutorials)
- [Terminal Interface Guide](game_controls/README.md)
- [Function Documentation](other/FUNCTION_DOCUMENTATION.md)

### ğŸ¤– Development & Training
- [Configuration](#ï¸-configuration)
- [Training Configuration Files](#training-configuration-files)
- [Troubleshooting & Support](#-troubleshooting--support)
- [Future Development & Improvements](#-future-development--improvements)
- [Contributing](#-contributing)

### ğŸ“š Research & Legal
- [References](#-references)
- [License & Legal Notice](#-license--legal-notice)
- [How to Cite This Work](#-how-to-cite-this-work)

---

## ğŸ² How Shadow Chase Works

Shadow Chase is an asymmetric pursuit-evasion game with imperfect information, inspired by Shadow Chase:

- **MrX (1 player)**: The fugitive who moves secretly around the city using taxi, bus, underground, and black tickets. MrX becomes visible only on certain turns and has special abilities like double moves.

- **Detectives (5 players)**: Work together to capture the MrX by landing on their position. They have limited transport tickets and their movements are always visible.

- **Victory Conditions**:
  - **Detectives win**: If any agent lands on MrX's position
  - **MrX wins**: If they evade capture for 24 turns or if Detectives run out of moves

- **Transport Network**: Players move through the city using three transport types:
  - ğŸš• **Taxi**: Available everywhere, limited tickets
  - ğŸšŒ **Bus**: Faster routes, fewer connections
  - ğŸš‡ **Underground**: Fastest but most limited network


## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- CUDA-compatible GPU (optional, for faster DQN training)

### Installation
```bash
git clone https://github.com/ettoremodina/ShadowChase.git
cd ShadowChase
pip install -r requirements.txt
```

### Three Main Ways to Use This Project

#### 1. ğŸ® Interactive Play (Graphical Interface)
Play Shadow Chase with a visual interface - control Detectives manually or watch AI agents play:
```bash
python main.py
```
- Choose between human and AI players
- Watch agent strategies in action
- Great for understanding game dynamics

![Shadow Chase Gameplay](IMMAGINI%20REPORT/image.png)

#### 2. ğŸ§ª Agent Testing & Comparison
Systematically test different agent combinations and analyze their performance:
```bash
python test_agents.py
```
- Runs all agent combinations automatically
- Generates detailed performance analysis
- Perfect for benchmarking and research

![Agent Win Rates](IMMAGINI%20REPORT/win_rates_by_agent.jpg)
![Comprehensive Dashboard](IMMAGINI%20REPORT/comprehensive_dashboard.jpg)

#### 3. ğŸ¤– Train New DQN Agents
Train your own Deep Q-Learning agents:
```bash
python train_dqn.py --role MrX --episodes 10000
python train_dqn.py --role detectives --episodes 10000
```
- Train either MrX or agent teams
- Supports CPU and GPU training
- Saves models for later use

![DQN Training Metrics](IMMAGINI%20REPORT/final_training_metrics_MrX.png)

## ğŸ—ï¸ Project Structure

```
ShadowChase/
â”œâ”€â”€ ğŸ“ agents/                    # AI agent implementations
â”‚   â”œâ”€â”€ base_agent.py            # Abstract base classes
â”‚   â”œâ”€â”€ random_agent.py          # Random baseline agents
â”‚   â”œâ”€â”€ heuristic_agent.py       # Distance-based strategy agents
â”‚   â”œâ”€â”€ mcts_agent.py            # Monte Carlo Tree Search agents
â”‚   â”œâ”€â”€ dqn_agent.py             # Deep Q-Learning agents
â”‚   â””â”€â”€ agent_registry.py        # Agent selection system
â”œâ”€â”€ ğŸ“ game_controls/             # Terminal-based gameplay
â”‚   â”œâ”€â”€ simple_game.py           # Command-line interface
â”‚   â”œâ”€â”€ game_logic.py            # Game flow control
â”‚   â””â”€â”€ display_utils.py         # Terminal formatting
â”œâ”€â”€ ğŸ“ training/                  # Training infrastructure
â”‚   â”œâ”€â”€ deep_q/                  # DQN-specific components
â”‚   â”œâ”€â”€ feature_extractor.py     # State representation
â”‚   â””â”€â”€ training_environment.py  # Standardized training interface
â”œâ”€â”€ ğŸ“ ShadowChase/              # Core game engine
â”‚   â”œâ”€â”€ core/                    # Game logic and rules
â”‚   â”œâ”€â”€ ui/                      # Graphical interface
â”‚   â””â”€â”€ services/                # Utilities and caching
â”œâ”€â”€ ğŸ“ data/                     # Game maps and configurations
â””â”€â”€ ğŸ“ papers/                   # Research literature
```

### Key Files

#### Main Entry Points
- **[main.py](main.py)**: Interactive graphical interface
- **[test_agents.py](test_agents.py)**: Agent testing and comparison
- **[train_dqn.py](train_dqn.py)**: Deep Q-Learning training
- **[game_controls/simple_game.py](game_controls/simple_game.py)**: Terminal interface

#### Configuration & Data
- **[training/configs/](training/configs/)**: Training configuration files
- **[data/](data/)**: Board data and game configurations
- **[requirements.txt](requirements.txt)**: Project dependencies

## ğŸ¤– Available Agent Types

### Basic Agents
- **Random Agent**: Makes random valid moves (baseline)
- **Heuristic Agent**: Uses distance-based strategies
  - MrX: Maximizes distance from closest agent
  - Detectives: Minimize distance to MrX's last known position

### Advanced AI Agents
- **MCTS Agent**: Monte Carlo Tree Search with random simulations
- **Optimized MCTS**: Enhanced with caching for better performance  
- **Epsilon-Greedy MCTS**: MCTS with heuristic-guided simulations
- **DQN Agent**: Deep Q-Learning neural networks

### Performance Notes
- **Cache system works well for MCTS** agents, providing significant speedup
- **Game method caching** is less effective (retrieval cost > recalculation cost)
- **DQN training** benefits from GPU acceleration but runs fine on CPU
- **Memory requirements**: 8GB+ RAM recommended for large training sessions
- **Training time**: DQN training can take several hours for 10,000+ episodes


## ğŸ® Usage Examples

### Run a Quick Test
```bash
# Play 10 games between DQN and random agents
python test_agents.py
# Results saved to debug/ folder with analysis graphs
```

### Train a New Agent
```bash
# Train MrX agent for 5000 episodes
python train_dqn.py --role MrX --episodes 5000 --map extracted

# Train detective agents for 5000 episodes
python train_dqn.py --role detectives --episodes 5000 --save-dir my_training
```

### Batch Analysis
```bash
# Terminal gameplay - 50 automated games
python game_controls/simple_game.py --batch 50 --map-size extracted --detectives 5
```

## ğŸš§ Examples & Tutorials

### Quick Start Examples
- **[main.py](main.py)**: Interactive graphical gameplay with manual or AI control
- **[test_agents.py](test_agents.py)**: Systematic agent comparison and benchmarking
- **[train_dqn.py](train_dqn.py)**: Deep Q-Learning agent training

### Terminal Interface Examples
- **[simple_game.py](game_controls/simple_game.py)**: Command-line gameplay

### Advanced Examples
- **[Example Games](ShadowChase/examples/example_games.py)**: Different game configurations
- **[Board Creation](other/createBoard.py)**: Interactive board creation tool
- **[Performance Analysis](other/profile_mcts_agent.py)**: MCTS agent profiling

## âš™ï¸ Configuration

### Training Configuration Files

The project includes several configuration files to customize training and game parameters:

- **[DQN Configuration](training/configs/dqn_config.json)**: Deep Q-Learning network architecture, training parameters, and hyperparameters
- **[MCTS Configuration](training/configs/mcts_config.json)**: Monte Carlo Tree Search parameters and optimization settings
- **[Board Metadata](data/board_metadata.json)**: Game board configuration and transport network settings
- **[Transport Colors](data/transport_colors.json)**: Visualization colors for different transport types

### Training Configuration
Edit `training/configs/dqn_config.json` to adjust:
- Network architecture (hidden layers, dropout)
- Training parameters (learning rate, epsilon decay)
- Experience replay settings

### Cache Settings
The caching system can be configured in your scripts:
```python
from ShadowChase.services.cache_system import *

enable_cache()
enable_namespace_cache(CacheNamespace.MCTS_NODES)      # Good for MCTS
disable_namespace_cache(CacheNamespace.GAME_METHODS)   # Not helpful
```

### Game Configuration
You can customize game parameters in several ways:
- **Map size**: Choose between test (10 nodes), full (199 nodes), or extracted board
- **Number of detectives**: Typically 2-5 detectives
- **Agent types**: Select from Random, Heuristic, MCTS, or DQN agents
- **Verbosity levels**: Control output detail in terminal mode

## ğŸš§ Future Development & Improvements

### High Priority Improvements Needed
- **ğŸ¨ User Interface Overhaul**: The current GUI needs significant improvement for better usability
- **ğŸ¤ Adversarial Training**: Implement adversarial learning to train Deep Learning based agents
- **ğŸ”„ Advanced RL Techniques**: Add PPO, A3C, or other modern RL algorithms
- **ğŸ“Š Better Evaluation**: More sophisticated metrics and analysis tools
- **ğŸ§ PettingZoo Integration**: *Currently in development* - Adding PettingZoo library support for standardized multi-agent reinforcement learning environments

### Potential Enhancements
- **ğŸ¯ Curriculum Learning**: Progressive difficulty in training scenarios
- **ğŸ”— Multi-Agent RL**: Cooperative learning for agent teams
- **ğŸ“ˆ Real-time Visualization**: Live training progress and agent behavior
- **ğŸŒ Web Interface**: Browser-based gameplay and training monitoring

### Research Opportunities
- **Game Theory Analysis**: Nash equilibria and optimal strategies
- **Transfer Learning**: Adapting agents to different map sizes

## ğŸ¤ Contributing

We welcome contributions! Areas where help is especially needed:

1. **UI/UX Improvements**: The current interface needs major enhancements
2. **Advanced RL Algorithms**: Implementing state-of-the-art techniques
3. **Performance Optimization**: Speeding up training and evaluation
4. **Documentation**: Tutorials, examples, and general documentation are still lacking
5. **Testing**: Unit tests and integration tests

## ğŸ“š References

The `papers/` directory contains relevant research literature on:
- Pursuit-evasion games, Scotland Yard, and strategy analysis
- Monte Carlo Tree Search applications
- Deep reinforcement learning for multi-agent systems
- Game theory and optimal strategies

## ğŸ“„ License & Legal Notice

### Project License
This project's **source code** is open source under the MIT License. See LICENSE file for details.

### Important Legal Disclaimer
This is an **independent implementation** inspired by the Scotland Yard board game, created for educational and research purposes. This project:
- âœ… **Educational/Research purposes only** - Not for commercial use
- âœ… **Independent implementation** - Original code and AI research
- âœ… **Open source** - Freely available for academic use
- âœ… **Inspired by Scotland Yard** - Acknowledges the original game concept
- âŒ **NOT affiliated** with Ravensburger AG or official Scotland Yard
- âŒ **Does NOT include** any copyrighted artwork or official assets

### Usage Restrictions
- **Academic & Research Use**: âœ… Encouraged for learning RL, game theory, and AI
- **Commercial Use**: âœ… Allowed with proper attribution (see citation below)
- **Distribution**: Feel free to share and modify
- **Attribution**: Please cite this project in academic work

### For Commercial Use
This project uses original content and can be used commercially:
1. Maintain attribution to the original author
2. Follow the MIT License terms
3. Consider contributing improvements back to the community
4. Ensure compliance with any dependencies' licenses

This project exists purely for educational and research purposes in artificial intelligence and game theory.

## ğŸ“– How to Cite This Work

If you use this project in your research or academic work, please cite it as:

### BibTeX
```bibtex
@software{modina2025ShadowChase,
  author = {Modina, Ettore},
  title = {Shadow Chase: AI Agent Training \& Evaluation Platform},
  year = {2025},
  url = {https://github.com/ettoremodina/ShadowChase},
  note = {Educational implementation inspired by Shadow Chase for reinforcement learning and game theory research}
}
```

### APA Style
Modina, E. (2025). *Shadow Chase: AI Agent Training & Evaluation Platform* [Computer software]. https://github.com/ettoremodina/ShadowChase

### IEEE Style
E. Modina, "Shadow Chase: AI Agent Training & Evaluation Platform," 2025. [Online]. Available: https://github.com/ettoremodina/ShadowChase


---

*Happy hunting, detectives! ğŸ•µï¸â€â™‚ï¸*